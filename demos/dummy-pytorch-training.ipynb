{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import time\n",
    "import subprocess\n",
    "import threading\n",
    "import re\n",
    "from queue import Queue\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Configurable parameters\n",
    "dataset_size = 100000\n",
    "batch_sizes = [128, 256, 512]  # Adjust for 2 GPUs (e.g., [256, 512, 1024] for H100)\n",
    "hidden_dim = 512\n",
    "num_epochs = 20\n",
    "num_workers = 4\n",
    "monitor_interval = 5\n",
    "learning_rate = 0.001\n",
    "lr_step_size = 5\n",
    "lr_gamma = 0.5\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Using device: {device}, Number of GPUs: {num_gpus}\")\n",
    "\n",
    "# Step 1: Synthetic dataset with learnable patterns\n",
    "class SyntheticImageDataset(Dataset):\n",
    "    def __init__(self, num_samples=dataset_size, img_size=28, num_classes=10):\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "        self.data = torch.randn(num_samples, 1, img_size, img_size)\n",
    "        self.labels = torch.randint(0, num_classes, (num_samples,))\n",
    "        for i in range(num_samples):\n",
    "            label = self.labels[i]\n",
    "            row_start = (label % 4) * (img_size // 4)\n",
    "            col_start = (label // 4) * (img_size // 4)\n",
    "            region_size = img_size // 5\n",
    "            self.data[i, 0, row_start:row_start+region_size, col_start:col_start+region_size] += label * 0.5\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "dataset = SyntheticImageDataset()\n",
    "\n",
    "# Step 2: CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, hidden_dim=hidden_dim, num_classes=10, input_size=28):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, hidden_dim // 4, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim // 4, hidden_dim // 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(hidden_dim // 2, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        fc1_input = hidden_dim * (input_size // 8) * (input_size // 8)\n",
    "        self.fc1 = nn.Linear(fc1_input, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.pool(self.conv1(x)))\n",
    "        x = self.relu(self.pool(self.conv2(x)))\n",
    "        x = self.relu(self.pool(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "if num_gpus > 1:\n",
    "    model = nn.DataParallel(model)  # Distribute across GPUs\n",
    "\n",
    "# Step 3: Optimizer, scheduler, loss, scaler\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_step_size, gamma=lr_gamma)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Step 4: GPU monitoring thread (handles multiple GPUs)\n",
    "def monitor_gpu(queue):\n",
    "    util_samples = []\n",
    "    mem_samples = []\n",
    "    while True:\n",
    "        try:\n",
    "            output = subprocess.check_output(\n",
    "                [\"nvidia-smi\", \"--query-gpu=utilization.gpu,memory.used\", \"--format=csv,noheader,nounits\"]\n",
    "            ).decode(\"utf-8\").strip()\n",
    "            # Parse each GPU's metrics\n",
    "            lines = output.split('\\n')\n",
    "            gpu_utils = []\n",
    "            gpu_mems = []\n",
    "            for line in lines:\n",
    "                util, mem = map(float, re.findall(r'\\d+', line))\n",
    "                gpu_utils.append(util)\n",
    "                gpu_mems.append(mem)\n",
    "            # Average across GPUs for this sample\n",
    "            avg_util = sum(gpu_utils) / len(gpu_utils)\n",
    "            avg_mem = sum(gpu_mems) / len(gpu_mems)\n",
    "            util_samples.append(avg_util)\n",
    "            mem_samples.append(avg_mem)\n",
    "            print(f\"GPU Util (avg): {avg_util:.1f}%, Mem (avg): {avg_mem:.1f} MiB, Per GPU: {list(zip(gpu_utils, gpu_mems))}\")\n",
    "        except Exception as e:\n",
    "            print(f\"GPU monitor error: {e}\")\n",
    "        if not queue.empty():\n",
    "            break\n",
    "        time.sleep(monitor_interval)\n",
    "    if util_samples and mem_samples:\n",
    "        avg_util = sum(util_samples) / len(util_samples)\n",
    "        avg_mem = sum(mem_samples) / len(mem_samples)\n",
    "        print(f\"Average GPU Utilization (all GPUs): {avg_util:.2f}%, Average Memory Used (all GPUs): {avg_mem:.2f} MiB\")\n",
    "    else:\n",
    "        print(\"No GPU samples collected.\")\n",
    "\n",
    "queue = Queue()\n",
    "monitor_thread = threading.Thread(target=monitor_gpu, args=(queue,))\n",
    "monitor_thread.start()\n",
    "\n",
    "# Step 5: Training loop with dynamic batch size\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    batch_size = batch_sizes[min(epoch // 5, len(batch_sizes) - 1)]\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} (Batch Size: {batch_size})\", leave=True)\n",
    "    for data, labels in progress_bar:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Stop monitoring\n",
    "queue.put(\"stop\")\n",
    "monitor_thread.join()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
