{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd3b0e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819fe8b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Parameters to tune for memory exhaustion (start small, increase)\n",
    "batch_size = 100  # Large batch: activations scale with this\n",
    "input_size = 100  # Wide inputs: increases weight matrices\n",
    "hidden_units = 1000  # Wide layers: GBs for weights/activations\n",
    "num_layers = 5  # More layers: deeper activations/gradients\n",
    "num_classes = 10\n",
    "num_samples = 5000  # Large dataset: loaded to GPU\n",
    "epochs = 5  # Run multiple epochs for sustained usage\n",
    "\n",
    "# Generate synthetic data (large tensors on GPU)\n",
    "with tf.device('/GPU:0'):\n",
    "    x_train = tf.random.normal((num_samples, input_size))\n",
    "    y_train = tf.random.uniform((num_samples,), minval=0, maxval=num_classes, dtype=tf.int32)\n",
    "    y_train = tf.one_hot(y_train, num_classes)  # One-hot for more memory\n",
    "\n",
    "# Build large model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(input_size,)))\n",
    "for _ in range(num_layers):\n",
    "    model.add(tf.keras.layers.Dense(hidden_units, activation='relu'))  # Dense layers eat memory\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile with optimizer/loss (Adam uses extra memory for states)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Dataset for batched training (prefetch to GPU)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Training loop to exhaust memory during backprop\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "history = model.fit(dataset, epochs=epochs, verbose=1)\n",
    "\n",
    "# Post-training memory check\n",
    "mem_info = tf.config.experimental.get_memory_info('GPU:0')\n",
    "print(f\"Peak used memory: {mem_info['peak'] / 1024**3:.2f} GB\")\n",
    "print(f\"Training time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Optional: Release memory\n",
    "# tf.keras.backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
