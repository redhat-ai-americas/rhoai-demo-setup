- name: Auto-clone MachineSet for GPU node deployment
  hosts: localhost
  gather_facts: no
  vars:
    # g6e.12xlarge NVIDIA L40S 4 GPUs vRAM 192GB
    # g6e.4xlarge NVIDIA L40S 1 GPU vRAM 48GB
    # g6.12xlarge NVIDIA L4 4 GPU vRAM 96GB
    gpu_instance_type: g6.12xlarge
    gpu_node_label: "node-role.kubernetes.io/gpu"
    namespace: openshift-machine-api

  tasks:
    - name: Get all MachineSets
      shell: oc get machineset -n {{ namespace }} -o json
      register: machinesets_json

    - name: Print all MachineSet names
      debug:
        msg: "MachineSets: {{ machinesets_json.stdout | from_json | json_query('items[*].metadata.name') }}"

    - name: Get all availability zones from MachineSets
      set_fact:
        availability_zones: "{{ machinesets_json.stdout | from_json | json_query('items[*].spec.template.spec.providerSpec.value.placement.availabilityZone') }}"

    - name: Print all availability zones
      debug:
        msg: "Availability Zones: {{ availability_zones }}"

    - name: Select first MachineSet as source (excluding GPU ones)
      set_fact:
        source_machineset: "{{ machinesets_json.stdout | from_json | json_query('items[?!contains(metadata.name, `gpu`)].metadata.name') | first }}"

    - name: Print selected source MachineSet
      debug:
        msg: "Selected source MachineSet: {{ source_machineset }}"

    - name: Generate unique ID for GPU MachineSet
      shell: date +%s
      register: timestamp_result

    - name: Define new GPU MachineSet name
      set_fact:
        gpu_machineset: "{{ source_machineset | regex_replace('worker', 'gpu') }}-{{ timestamp_result.stdout }}"

    - name: Print new GPU MachineSet name
      debug:
        msg: "New GPU MachineSet name: {{ gpu_machineset }}"

    - name: Get source MachineSet YAML
      command: >
        oc get machineset -n {{ namespace }} {{ source_machineset }} -o yaml
      register: machineset_yaml_raw

    - name: Modify MachineSet YAML for GPU node
      copy:
        content: "{{ machineset_yaml_raw.stdout
          | from_yaml
          | combine({
          'metadata': {
          'name': gpu_machineset,
          'labels': {
          'machine.openshift.io/cluster-api-machineset': gpu_machineset,
          'cluster-api/accelerator': 'nvidia-gpu'
          }
          },
          'spec': {
          'replicas': 1,
          'selector': {
          'matchLabels': {
          'machine.openshift.io/cluster-api-machineset': gpu_machineset
          }
          },
          'template': {
          'metadata': {
          'labels': {
          'machine.openshift.io/cluster-api-machineset': gpu_machineset,
          (gpu_node_label): ''
          }
          },
          'spec': {
          'metadata': {
          'labels': {
          (gpu_node_label): '',
          'cluster-api/accelerator': 'nvidia-gpu'
          }
          },
          'providerSpec': {
          'value': machineset_yaml_raw.stdout | from_yaml
          | json_query('spec.template.spec.providerSpec.value')
          | combine({
          'instanceType': gpu_instance_type
          })
          }
          }
          }
          }
          }, recursive=True)
          | to_nice_yaml }}"
        dest: /tmp/gpu-machineset.yaml

    - name: Apply GPU MachineSet
      command: oc apply -f /tmp/gpu-machineset.yaml

    - name: Wait for GPU node to join cluster
      retries: 20
      delay: 30
      shell: >
        oc get nodes -l {{ gpu_node_label }} --no-headers | grep -i Ready | wc -l
      register: gpu_node_count
      until: gpu_node_count.stdout | int >= 1
      changed_when: false

    - name: Display GPU node info
      shell: oc get nodes -l {{ gpu_node_label }} -o wide
      register: gpu_node_output

    - name: Show final result
      debug:
        msg: "{{ gpu_node_output.stdout }}"

    - name: Wait for NVIDIA GPU operator pods to be Running
      shell: |
        required_pods="nvidia-container-toolkit-daemonset nvidia-driver-daemonset nvidia-device-plugin-daemonset nvidia-operator-validator"
        not_ready=""
        echo "Pod status summary:"
        for pod_pattern in $required_pods; do
          total=$(oc get pods -n nvidia-gpu-operator --no-headers | grep "$pod_pattern" | wc -l)
          running=$(oc get pods -n nvidia-gpu-operator --no-headers | grep "$pod_pattern" | grep Running | wc -l)
          echo "  $pod_pattern: $running/$total Running"
          if [ "$total" -eq 0 ] || [ "$running" -lt "$total" ]; then
            not_ready="$not_ready $pod_pattern"
          fi
        done
        if [ -z "$not_ready" ]; then
          echo "All required NVIDIA pods are Running."
          exit 0
        else
          echo "Pods not ready:$not_ready"
          exit 1
        fi
      register: nvidia_pods_ready
      retries: 40 # 40 x 30s = 1200s = 20min
      delay: 30
      until: nvidia_pods_ready.rc == 0
      changed_when: false

    - name: Print status of NVIDIA GPU operator pods
      shell: |
        required_pods="nvidia-container-toolkit-daemonset nvidia-driver-daemonset nvidia-device-plugin-daemonset nvidia-operator-validator"
        echo "Pod status summary:"
        for pod_pattern in $required_pods; do
          oc get pods -n nvidia-gpu-operator --no-headers | grep "$pod_pattern" || echo "No pod found for $pod_pattern"
        done
      register: nvidia_pods_status
      changed_when: false
      ignore_errors: true

    - name: Print each NVIDIA pod status line
      debug:
        msg: "{{ item }}"
      loop: "{{ nvidia_pods_status.stdout.split('\n') }}"
